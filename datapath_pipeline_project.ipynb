{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c387556-aa2f-45dd-a615-fa49b176a7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 install --upgrade --quiet --user google-cloud-aiplatform  \\\n",
    "                                 google-cloud-storage \\\n",
    "                                 kfp \\\n",
    "                                 google-cloud-pipeline-components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e5b18f4-0b72-4bc7-a0ce-4a699a2f7cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "# Define project\n",
    "PROJECT_ID = \"learning-project-38730\"  # @param {type:\"string\"}\n",
    "\n",
    "# Set the project id\n",
    "! gcloud config set project {PROJECT_ID}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a0f51e3-7b6c-4c6f-94e8-b09f6604de63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define region\n",
    "REGION = \"us-central1\"  # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38a6820d-d968-4f42-a3f0-062ce98d87a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://bucket-datapath-project\n"
     ]
    }
   ],
   "source": [
    "# Define bucket\n",
    "BUCKET_NAME = \"bucket-datapath-project\"  # @param {type:\"string\"}\n",
    "BUCKET_URI = f\"gs://{BUCKET_NAME}\"\n",
    "\n",
    "! echo $BUCKET_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "422afe81-d1c6-4c22-bfa0-9e64df83f5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bucket\n",
    "# ! gsutil mb -l $REGION -p $PROJECT_ID $BUCKET_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee7406a1-553c-4b89-bdb5-03f2e7f2cadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "SERVICE_ACCOUNT = \"datapath-bootcamp@learning-project-38730.iam.gserviceaccount.com\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f776451-6fba-4c76-a9dd-69438438ddf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectCreator $BUCKET_URI\n",
    "\n",
    "# ! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectViewer $BUCKET_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42f8755a-66ff-49ef-aefc-856ff9de6dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import json\n",
    "import google.cloud.aiplatform as aiplatform\n",
    "import google.cloud.aiplatform as aip\n",
    "from kfp import compiler, dsl\n",
    "from kfp.dsl import component\n",
    "from typing import List\n",
    "from kfp import client\n",
    "from kfp import dsl\n",
    "from kfp.dsl import Dataset\n",
    "from kfp.dsl import Input\n",
    "from kfp.dsl import Model\n",
    "from kfp.dsl import Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17ac6348-424f-4725-a185-9755568499e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vertex AI Pipelines constants\n",
    "PIPELINE_ROOT = \"{}/pipeline_root/control\".format(BUCKET_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73c8033e-bfb8-4cbf-893d-5683bf988b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Vertex AI SDK for Python\n",
    "aiplatform.init(project=PROJECT_ID, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44af4cbb-eef2-4361-bf95-14e2480fc17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create components\n",
    "@dsl.component(packages_to_install=['pandas==1.3.5'])\n",
    "def create_dataset(iris_dataset: Output[Dataset]):\n",
    "    import pandas as pd\n",
    "\n",
    "    csv_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'\n",
    "    col_names = [\n",
    "        'Sepal_Length', 'Sepal_Width', 'Petal_Length', 'Petal_Width', 'Labels'\n",
    "    ]\n",
    "    df = pd.read_csv(csv_url, names=col_names)\n",
    "\n",
    "    with open(iris_dataset.path, 'w') as f:\n",
    "        df.to_csv(f)\n",
    "\n",
    "\n",
    "@dsl.component(packages_to_install=['pandas==1.3.5', 'scikit-learn==1.0.2'])\n",
    "def normalize_dataset(\n",
    "    input_iris_dataset: Input[Dataset],\n",
    "    normalized_iris_dataset: Output[Dataset],\n",
    "    standard_scaler: bool,\n",
    "    min_max_scaler: bool,\n",
    "):\n",
    "    if standard_scaler is min_max_scaler:\n",
    "        raise ValueError(\n",
    "            'Exactly one of standard_scaler or min_max_scaler must be True.')\n",
    "\n",
    "    import pandas as pd\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "    with open(input_iris_dataset.path) as f:\n",
    "        df = pd.read_csv(f)\n",
    "    labels = df.pop('Labels')\n",
    "\n",
    "    if standard_scaler:\n",
    "        scaler = StandardScaler()\n",
    "    if min_max_scaler:\n",
    "        scaler = MinMaxScaler()\n",
    "\n",
    "    df = pd.DataFrame(scaler.fit_transform(df))\n",
    "    df['Labels'] = labels\n",
    "    with open(normalized_iris_dataset.path, 'w') as f:\n",
    "        df.to_csv(f)\n",
    "\n",
    "\n",
    "@dsl.component(packages_to_install=['pandas==1.3.5', 'scikit-learn==1.0.2'])\n",
    "def train_model(\n",
    "    normalized_iris_dataset: Input[Dataset],\n",
    "    model: Output[Model],\n",
    "    n_neighbors: int,\n",
    "):\n",
    "    import pickle\n",
    "\n",
    "    import pandas as pd\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "    with open(normalized_iris_dataset.path) as f:\n",
    "        df = pd.read_csv(f)\n",
    "\n",
    "    y = df.pop('Labels')\n",
    "    X = df\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "    clf = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "    clf.fit(X_train, y_train)\n",
    "    with open(model.path, 'wb') as f:\n",
    "        pickle.dump(clf, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9a5f95d-7f47-4620-9c6a-ace909c9b0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate pipeline\n",
    "@dsl.pipeline(name='iris-training-pipeline')\n",
    "def my_pipeline(\n",
    "    standard_scaler: bool,\n",
    "    min_max_scaler: bool,\n",
    "    neighbors: List[int],\n",
    "):\n",
    "    create_dataset_task = create_dataset()\n",
    "\n",
    "    normalize_dataset_task = normalize_dataset(\n",
    "        input_iris_dataset=create_dataset_task.outputs['iris_dataset'],\n",
    "        standard_scaler=True,\n",
    "        min_max_scaler=False)\n",
    "\n",
    "    with dsl.ParallelFor(neighbors) as n_neighbors:\n",
    "        train_model(\n",
    "            normalized_iris_dataset=normalize_dataset_task\n",
    "            .outputs['normalized_iris_dataset'],\n",
    "            n_neighbors=n_neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4dfa11a0-6b1e-4caf-a9ae-8426543268ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline compiled successfully! Check iris_training_pipeline.yaml for the YAML definition.\n"
     ]
    }
   ],
   "source": [
    "# Compile\n",
    "pipeline_filename = 'iris_training_pipeline.yaml'\n",
    "compiler.Compiler().compile(my_pipeline, pipeline_filename)\n",
    "print(f'Pipeline compiled successfully! Check {pipeline_filename} for the YAML definition.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadf548f-de86-4a91-959e-3a4e02699f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineJob created. Resource name: projects/394727607809/locations/us-central1/pipelineJobs/iris-training-pipeline-20231123150340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/394727607809/locations/us-central1/pipelineJobs/iris-training-pipeline-20231123150340\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To use this PipelineJob in another session:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline_job = aiplatform.PipelineJob.get('projects/394727607809/locations/us-central1/pipelineJobs/iris-training-pipeline-20231123150340')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/394727607809/locations/us-central1/pipelineJobs/iris-training-pipeline-20231123150340')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/iris-training-pipeline-20231123150340?project=394727607809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/iris-training-pipeline-20231123150340?project=394727607809\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineJob projects/394727607809/locations/us-central1/pipelineJobs/iris-training-pipeline-20231123150340 current state:\n",
      "PipelineState.PIPELINE_STATE_PENDING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/394727607809/locations/us-central1/pipelineJobs/iris-training-pipeline-20231123150340 current state:\n",
      "PipelineState.PIPELINE_STATE_PENDING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineJob projects/394727607809/locations/us-central1/pipelineJobs/iris-training-pipeline-20231123150340 current state:\n",
      "PipelineState.PIPELINE_STATE_PENDING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/394727607809/locations/us-central1/pipelineJobs/iris-training-pipeline-20231123150340 current state:\n",
      "PipelineState.PIPELINE_STATE_PENDING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineJob projects/394727607809/locations/us-central1/pipelineJobs/iris-training-pipeline-20231123150340 current state:\n",
      "PipelineState.PIPELINE_STATE_PENDING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/394727607809/locations/us-central1/pipelineJobs/iris-training-pipeline-20231123150340 current state:\n",
      "PipelineState.PIPELINE_STATE_PENDING\n"
     ]
    }
   ],
   "source": [
    "DISPLAY_NAME = \"control\"\n",
    "\n",
    "job = aip.PipelineJob(\n",
    "    display_name=DISPLAY_NAME,\n",
    "    template_path=\"iris_training_pipeline.yaml\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    parameter_values={\n",
    "        'min_max_scaler': True,\n",
    "        'standard_scaler': False,\n",
    "        'neighbors': [3, 6, 9]\n",
    "    }\n",
    ")\n",
    "\n",
    "job.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b917acf4-5c71-4ed1-b22f-7d328dc58728",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_endpoint_sample(\n",
    "    project: str,\n",
    "    display_name: str,\n",
    "    location: str,\n",
    "):\n",
    "    aiplatform.init(project=project, location=location)\n",
    "\n",
    "    endpoint = aiplatform.Endpoint.create(\n",
    "        display_name=display_name,\n",
    "        project=project,\n",
    "        location=location,\n",
    "    )\n",
    "\n",
    "    print(endpoint.display_name)\n",
    "    print(endpoint.resource_name)\n",
    "    return endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cf3969da-0530-460e-9cf9-8b151383f2cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Endpoint\n",
      "Create Endpoint backing LRO: projects/394727607809/locations/us-central1/endpoints/4254619617307131904/operations/3669423726157365248\n",
      "Endpoint created. Resource name: projects/394727607809/locations/us-central1/endpoints/4254619617307131904\n",
      "To use this Endpoint in another session:\n",
      "endpoint = aiplatform.Endpoint('projects/394727607809/locations/us-central1/endpoints/4254619617307131904')\n",
      "iris_endpoint\n",
      "projects/394727607809/locations/us-central1/endpoints/4254619617307131904\n"
     ]
    }
   ],
   "source": [
    "endpoint = create_endpoint_sample(project= PROJECT_ID, \\\n",
    "                                    display_name= 'iris_endpoint', \\\n",
    "                                    location= REGION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5e856b0c-090d-4a84-9e88-d63ba849c2f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://us-central1-aiplatform.googleapis.com/]\n",
      "ENDPOINT_ID          DISPLAY_NAME\n",
      "4254619617307131904  iris_endpoint\n",
      "7045725486369996800  iris_endpoint\n"
     ]
    }
   ],
   "source": [
    "ENDPOINT_NAME = 'iris_endpoint'\n",
    "!gcloud ai endpoints list \\\n",
    "  --region=$REGION\\\n",
    "  --filter=display_name=$ENDPOINT_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5d8716-65be-4b00-bace-ff47ee818ed2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Local)",
   "language": "python",
   "name": "local-base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
